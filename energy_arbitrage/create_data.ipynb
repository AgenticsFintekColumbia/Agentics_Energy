{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03773ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "# Import your existing framework components\n",
    "from agentic_energy.schemas import BatteryParams, DayInputs, SolveRequest, SolveResponse\n",
    "from agentic_energy.data_loader import BatteryDataLoader\n",
    "from agentic_energy.milp.milp_mcp_server import solve_daily_milp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2986ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CONFIG = {\n",
    "    'italy': {\n",
    "        'file': 'Italy_data.csv',\n",
    "        'train_years': [2022],\n",
    "        'test_years': [2023],\n",
    "        'price_column': 'prices',  # Adjust based on your CSV structure\n",
    "        'demand_column': 'consumption',  # Adjust based on your CSV structure\n",
    "        'timestamp_column': 'timestamps',  # Adjust based on your CSV structure\n",
    "    },\n",
    "    'germany': {\n",
    "        'file': 'Germany_energy_Data.csv',\n",
    "        'train_years': [2019, 2020, 2021, 2022],\n",
    "        'test_years': [2023],\n",
    "        'price_column': 'prices',\n",
    "        'demand_column': 'consumption',\n",
    "        'timestamp_column': 'timestamps',\n",
    "    },\n",
    "    'caiso': {\n",
    "        'file': 'CAISO_data.csv',\n",
    "        'train_years': [2021, 2022],\n",
    "        'test_years': [2023],\n",
    "        'price_column': 'prices',\n",
    "        'demand_column': 'consumption',\n",
    "        'timestamp_column': 'timestamps',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0197853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDatasetLoader:\n",
    "    \"\"\"Load and prepare data from multiple ISOs\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"./data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "    def load_dataset(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        split: str = 'train'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load dataset for specified split (train or test)\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: 'italy', 'germany', or 'caiso'\n",
    "            split: 'train' or 'test'\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with columns: timestamp, price, demand\n",
    "        \"\"\"\n",
    "        if dataset_name not in DATASET_CONFIG:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}. Use 'italy', 'germany', or 'caiso'\")\n",
    "        \n",
    "        config = DATASET_CONFIG[dataset_name]\n",
    "        file_path = self.data_dir / config['file']\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "        \n",
    "        # Load CSV\n",
    "        print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        if config['timestamp_column'] in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df[config['timestamp_column']])\n",
    "        else:\n",
    "            raise ValueError(f\"Timestamp column '{config['timestamp_column']}' not found in {file_path}\")\n",
    "        \n",
    "        # Rename columns to standard names\n",
    "        df = df.rename(columns={\n",
    "            config['price_column']: 'price',\n",
    "            config['demand_column']: 'demand'\n",
    "        })\n",
    "        \n",
    "        # Extract year\n",
    "        df['year'] = df['timestamp'].dt.year\n",
    "        \n",
    "        # Filter by split\n",
    "        if split == 'train':\n",
    "            years = config['train_years']\n",
    "        elif split == 'test':\n",
    "            years = config['test_years']\n",
    "        else:\n",
    "            raise ValueError(f\"Split must be 'train' or 'test', got: {split}\")\n",
    "        \n",
    "        df_filtered = df[df['year'].isin(years)].copy()\n",
    "        \n",
    "        print(f\"  Loaded {len(df_filtered)} records from years {years}\")\n",
    "        print(f\"  Date range: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}\")\n",
    "        print(f\"  Price range: {df_filtered['price'].min():.2f} - {df_filtered['price'].max():.2f}\")\n",
    "        print(f\"  Demand range: {df_filtered['demand'].min():.2f} - {df_filtered['demand'].max():.2f}\")\n",
    "        \n",
    "        return df_filtered[['timestamp', 'price', 'demand', 'year']].sort_values('timestamp')\n",
    "    \n",
    "    def get_daily_batches(self, df: pd.DataFrame) -> List[Tuple[str, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Split dataframe into daily batches (24-hour periods)\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples: (date_str, prices_array, demand_array)\n",
    "        \"\"\"\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        \n",
    "        daily_batches = []\n",
    "        for date, group in df.groupby('date'):\n",
    "            if len(group) == 24:  # Only complete days\n",
    "                date_str = str(date)\n",
    "                prices = group['price'].values\n",
    "                demand = group['demand'].values\n",
    "                daily_batches.append((date_str, prices, demand))\n",
    "            else:\n",
    "                print(f\"  Warning: Skipping {date} (only {len(group)} hours)\")\n",
    "        \n",
    "        print(f\"  Extracted {len(daily_batches)} complete days\")\n",
    "        return daily_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea41c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generates fine-tuning datasets from MILP solutions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, battery_config: Dict[str, float]):\n",
    "        self.battery_config = battery_config\n",
    "        self.instruction = self._create_instruction()\n",
    "        \n",
    "    def _create_instruction(self) -> str:\n",
    "        \"\"\"Create the static instruction component\"\"\"\n",
    "        return \"\"\"You are an energy storage optimization expert. Given electricity price forecasts and battery specifications, determine the optimal charge/discharge schedule for a 24-hour period to minimize operational costs while respecting all physical and operational constraints.\n",
    "\n",
    "Your task is to:\n",
    "1. Analyze the price forecast to identify charging opportunities (low prices) and discharging opportunities (high prices)\n",
    "2. Respect battery capacity (kWh), power limits (kW), efficiency losses, and state of charge (SOC) bounds\n",
    "3. Ensure SOC trajectory remains within [SOC_min, SOC_max] at all times\n",
    "4. Meet demand at every timestep\n",
    "5. Provide detailed reasoning for each hourly decision\n",
    "6. Output the complete schedule in JSON format\n",
    "\n",
    "Key decision principles:\n",
    "- Charge during hours with prices below average (especially if below 25th percentile)\n",
    "- Discharge during hours with prices above average (especially if above 75th percentile)\n",
    "- Account for round-trip efficiency losses (charge efficiency Ã— discharge efficiency)\n",
    "- Maintain feasible SOC trajectory throughout the day\n",
    "- Prioritize largest price arbitrage opportunities\n",
    "- Consider the time value of stored energy\n",
    "- Ensure smooth SOC transitions to avoid unnecessary cycling\n",
    "\n",
    "Output format:\n",
    "- Include complete charge/discharge schedule (kW for each hour)\n",
    "- Include SOC trajectory\n",
    "- Provide hourly decision reasoning\n",
    "- Calculate total operational cost\n",
    "- Validate all constraints are satisfied\"\"\"\n",
    "    \n",
    "    def create_input_component(\n",
    "        self,\n",
    "        date: str,\n",
    "        prices: np.ndarray,\n",
    "        demand: np.ndarray,\n",
    "        battery_params: BatteryParams,\n",
    "        dataset_name: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create the input component with all necessary context\n",
    "        \"\"\"\n",
    "        # Calculate price statistics\n",
    "        price_stats = {\n",
    "            \"min\": float(np.min(prices)),\n",
    "            \"max\": float(np.max(prices)),\n",
    "            \"mean\": float(np.mean(prices)),\n",
    "            \"median\": float(np.median(prices)),\n",
    "            \"std\": float(np.std(prices)),\n",
    "            \"p25\": float(np.percentile(prices, 25)),\n",
    "            \"p75\": float(np.percentile(prices, 75)),\n",
    "        }\n",
    "        \n",
    "        # Identify price patterns\n",
    "        peak_hours = [int(h) for h in range(24) if prices[h] > price_stats[\"p75\"]]\n",
    "        valley_hours = [int(h) for h in range(24) if prices[h] < price_stats[\"p25\"]]\n",
    "        \n",
    "        input_dict = {\n",
    "            \"date\": date,\n",
    "            \"region\": dataset_name.upper() if dataset_name else \"UNKNOWN\",\n",
    "            \"timestep\": \"hourly\",\n",
    "            \"horizon_hours\": 24,\n",
    "            \n",
    "            # Price data\n",
    "            \"price_forecast_eur_per_mwh\": prices.tolist(),\n",
    "            \"price_statistics\": price_stats,\n",
    "            \"price_patterns\": {\n",
    "                \"peak_hours\": peak_hours,\n",
    "                \"valley_hours\": valley_hours,\n",
    "                \"price_range\": float(price_stats[\"max\"] - price_stats[\"min\"]),\n",
    "                \"volatility_coefficient\": float(price_stats[\"std\"] / price_stats[\"mean\"]) if price_stats[\"mean\"] > 0 else 0.0\n",
    "            },\n",
    "            \n",
    "            # Demand data\n",
    "            \"demand_forecast_kw\": demand.tolist(),\n",
    "            \"demand_statistics\": {\n",
    "                \"min\": float(np.min(demand)),\n",
    "                \"max\": float(np.max(demand)),\n",
    "                \"mean\": float(np.mean(demand)),\n",
    "                \"total_daily_kwh\": float(np.sum(demand))\n",
    "            },\n",
    "            \n",
    "            # Battery specifications\n",
    "            \"battery_specifications\": {\n",
    "                \"capacity_kwh\": float(battery_params.capacity_kwh),\n",
    "                \"max_charge_power_kw\": float(battery_params.cmax_kw),\n",
    "                \"max_discharge_power_kw\": float(battery_params.dmax_kw),\n",
    "                \"charge_efficiency\": float(battery_params.eta_c),\n",
    "                \"discharge_efficiency\": float(battery_params.eta_d),\n",
    "                \"roundtrip_efficiency\": float(battery_params.eta_c * battery_params.eta_d),\n",
    "                \"initial_soc\": float(battery_params.soc_init),\n",
    "                \"soc_minimum\": float(battery_params.soc_min),\n",
    "                \"soc_maximum\": float(battery_params.soc_max),\n",
    "                \"soc_target_end_of_day\": float(battery_params.soc_target) if battery_params.soc_target is not None else None,\n",
    "            },\n",
    "            \n",
    "            # Operational constraints\n",
    "            \"operational_constraints\": {\n",
    "                \"timestep_duration_hours\": 1.0,\n",
    "                \"allow_grid_export\": False,\n",
    "                \"must_meet_demand_every_hour\": True,\n",
    "                \"simultaneous_charge_discharge\": False\n",
    "            },\n",
    "            \n",
    "            # Market context\n",
    "            \"market_context\": {\n",
    "                \"day_of_week\": datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%A\"),\n",
    "                \"season\": self._get_season(date),\n",
    "                \"expected_arbitrage_opportunities\": len(peak_hours) * len(valley_hours) > 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return json.dumps(input_dict, indent=2)\n",
    "    \n",
    "    def create_output_component(\n",
    "        self,\n",
    "        milp_solution: SolveResponse,\n",
    "        prices: np.ndarray,\n",
    "        demand: np.ndarray,\n",
    "        battery_params: BatteryParams\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create the output component from MILP solution with detailed reasoning\n",
    "        \"\"\"\n",
    "        # Extract MILP solution components\n",
    "        charge_schedule = milp_solution.charge_kw if milp_solution.charge_kw else [0] * 24\n",
    "        discharge_schedule = milp_solution.discharge_kw if milp_solution.discharge_kw else [0] * 24\n",
    "        soc_trajectory = milp_solution.soc if milp_solution.soc else [battery_params.soc_init] * 25\n",
    "        import_grid = milp_solution.import_kw if milp_solution.import_kw else demand.tolist()\n",
    "        export_grid = milp_solution.export_kw if milp_solution.export_kw else [0] * 24\n",
    "        total_cost = milp_solution.objective_cost if milp_solution.objective_cost else 0.0\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_price = np.mean(prices)\n",
    "        p25_price = np.percentile(prices, 25)\n",
    "        p75_price = np.percentile(prices, 75)\n",
    "        \n",
    "        # Generate hourly reasoning\n",
    "        hourly_decisions = []\n",
    "        for hour in range(24):\n",
    "            decision = self._analyze_hour_decision(\n",
    "                hour=hour,\n",
    "                price=prices[hour],\n",
    "                charge=charge_schedule[hour],\n",
    "                discharge=discharge_schedule[hour],\n",
    "                soc_before=soc_trajectory[hour],\n",
    "                soc_after=soc_trajectory[hour + 1] if hour + 1 < len(soc_trajectory) else soc_trajectory[hour],\n",
    "                avg_price=avg_price,\n",
    "                p25_price=p25_price,\n",
    "                p75_price=p75_price,\n",
    "                battery_params=battery_params,\n",
    "                demand=demand[hour]\n",
    "            )\n",
    "            hourly_decisions.append(decision)\n",
    "        \n",
    "        # Generate strategy summary\n",
    "        strategy_summary = self._generate_strategy_summary(\n",
    "            hourly_decisions, prices, charge_schedule, discharge_schedule, soc_trajectory\n",
    "        )\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        performance_metrics = self._calculate_performance_metrics(\n",
    "            charge_schedule, discharge_schedule, soc_trajectory,\n",
    "            prices, import_grid, export_grid, battery_params\n",
    "        )\n",
    "        \n",
    "        # Construct output\n",
    "        output_dict = {\n",
    "            \"optimization_status\": milp_solution.status if milp_solution.status else \"unknown\",\n",
    "            \"objective_value\": {\n",
    "                \"total_cost_eur\": float(total_cost),\n",
    "                \"average_hourly_cost_eur\": float(total_cost / 24) if total_cost else 0.0\n",
    "            },\n",
    "            \n",
    "            \"schedule\": {\n",
    "                \"charge_kw\": [float(c) for c in charge_schedule],\n",
    "                \"discharge_kw\": [float(d) for d in discharge_schedule],\n",
    "                \"soc_trajectory\": [float(s) for s in soc_trajectory],\n",
    "                \"grid_import_kw\": [float(i) for i in import_grid],\n",
    "                \"grid_export_kw\": [float(e) for e in export_grid]\n",
    "            },\n",
    "            \n",
    "            \"strategy_summary\": strategy_summary,\n",
    "            \n",
    "            \"hourly_decision_reasoning\": hourly_decisions,\n",
    "            \n",
    "            \"performance_metrics\": performance_metrics,\n",
    "            \n",
    "            \"validation\": {\n",
    "                \"all_constraints_satisfied\": self._validate_solution(\n",
    "                    charge_schedule, discharge_schedule, soc_trajectory, battery_params\n",
    "                ),\n",
    "                \"min_soc_value\": float(np.min(soc_trajectory)),\n",
    "                \"max_soc_value\": float(np.max(soc_trajectory)),\n",
    "                \"soc_violations\": int(np.sum([\n",
    "                    1 for s in soc_trajectory \n",
    "                    if s < battery_params.soc_min - 1e-6 or s > battery_params.soc_max + 1e-6\n",
    "                ])),\n",
    "                \"power_violations\": int(np.sum([\n",
    "                    1 for c, d in zip(charge_schedule, discharge_schedule)\n",
    "                    if c > battery_params.cmax_kw + 1e-6 or d > battery_params.dmax_kw + 1e-6\n",
    "                ]))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return json.dumps(output_dict, indent=2)\n",
    "    \n",
    "    def _analyze_hour_decision(\n",
    "        self,\n",
    "        hour: int,\n",
    "        price: float,\n",
    "        charge: float,\n",
    "        discharge: float,\n",
    "        soc_before: float,\n",
    "        soc_after: float,\n",
    "        avg_price: float,\n",
    "        p25_price: float,\n",
    "        p75_price: float,\n",
    "        battery_params: BatteryParams,\n",
    "        demand: float\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Generate detailed reasoning for a single hour's decision\"\"\"\n",
    "        \n",
    "        # Determine decision type\n",
    "        if charge > 0.01:\n",
    "            decision_type = \"CHARGE\"\n",
    "            action_magnitude = charge\n",
    "        elif discharge > 0.01:\n",
    "            decision_type = \"DISCHARGE\"\n",
    "            action_magnitude = discharge\n",
    "        else:\n",
    "            decision_type = \"IDLE\"\n",
    "            action_magnitude = 0.0\n",
    "        \n",
    "        # Generate reasoning\n",
    "        reasoning_parts = []\n",
    "        \n",
    "        # Price analysis\n",
    "        if decision_type == \"CHARGE\":\n",
    "            if price < p25_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is in the bottom quartile (below {p25_price:.2f}), \"\n",
    "                    f\"making this an excellent charging opportunity.\"\n",
    "                )\n",
    "            elif price < avg_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is below average ({avg_price:.2f}), \"\n",
    "                    f\"making this a good charging window.\"\n",
    "                )\n",
    "            reasoning_parts.append(\n",
    "                f\"Charging at {charge:.2f} kW to store energy for later use during more expensive hours.\"\n",
    "            )\n",
    "            \n",
    "        elif decision_type == \"DISCHARGE\":\n",
    "            if price > p75_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is in the top quartile (above {p75_price:.2f}), \"\n",
    "                    f\"making this an excellent discharging opportunity.\"\n",
    "                )\n",
    "            elif price > avg_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is above average ({avg_price:.2f}), \"\n",
    "                    f\"making this a good discharging window.\"\n",
    "                )\n",
    "            reasoning_parts.append(\n",
    "                f\"Discharging at {discharge:.2f} kW to reduce grid imports during expensive hours.\"\n",
    "            )\n",
    "            \n",
    "        else:  # IDLE\n",
    "            price_position = \"near average\" if abs(price - avg_price) < 0.1 * avg_price else \"moderate\"\n",
    "            reasoning_parts.append(\n",
    "                f\"Price {price:.2f} EUR/MWh is {price_position} ({avg_price:.2f} average), \"\n",
    "                f\"not offering sufficient arbitrage opportunity to justify battery action.\"\n",
    "            )\n",
    "        \n",
    "        # SOC constraints\n",
    "        soc_change = soc_after - soc_before\n",
    "        if abs(soc_change) > 0.001:\n",
    "            reasoning_parts.append(\n",
    "                f\"SOC changes from {soc_before:.1%} to {soc_after:.1%} \"\n",
    "                f\"({'increasing' if soc_change > 0 else 'decreasing'} by {abs(soc_change):.1%}).\"\n",
    "            )\n",
    "        \n",
    "        # Constraint boundaries\n",
    "        if soc_after >= battery_params.soc_max - 0.01:\n",
    "            reasoning_parts.append(\"Battery reaches maximum SOC capacity.\")\n",
    "        elif soc_after <= battery_params.soc_min + 0.01:\n",
    "            reasoning_parts.append(\"Battery reaches minimum SOC limit.\")\n",
    "        \n",
    "        if decision_type == \"CHARGE\" and charge >= battery_params.cmax_kw - 0.01:\n",
    "            reasoning_parts.append(\"Charging at maximum power capacity.\")\n",
    "        elif decision_type == \"DISCHARGE\" and discharge >= battery_params.dmax_kw - 0.01:\n",
    "            reasoning_parts.append(\"Discharging at maximum power capacity.\")\n",
    "        \n",
    "        # Energy balance\n",
    "        net_demand = demand - discharge + charge\n",
    "        reasoning_parts.append(f\"Net grid import: {net_demand:.2f} kW to meet {demand:.2f} kW demand.\")\n",
    "        \n",
    "        return {\n",
    "            \"hour\": hour,\n",
    "            \"time_of_day\": f\"{hour:02d}:00\",\n",
    "            \"electricity_price_eur_per_mwh\": float(price),\n",
    "            \"price_vs_average\": float(price - avg_price),\n",
    "            \"decision\": decision_type,\n",
    "            \"charge_power_kw\": float(charge),\n",
    "            \"discharge_power_kw\": float(discharge),\n",
    "            \"soc_before_action\": float(soc_before),\n",
    "            \"soc_after_action\": float(soc_after),\n",
    "            \"soc_change\": float(soc_change),\n",
    "            \"detailed_reasoning\": \" \".join(reasoning_parts)\n",
    "        }\n",
    "    \n",
    "    def _generate_strategy_summary(\n",
    "        self,\n",
    "        hourly_decisions: List[Dict],\n",
    "        prices: np.ndarray,\n",
    "        charge_schedule: List[float],\n",
    "        discharge_schedule: List[float],\n",
    "        soc_trajectory: List[float]\n",
    "    ) -> str:\n",
    "        \"\"\"Generate high-level strategy summary\"\"\"\n",
    "        \n",
    "        charge_hours = [d['hour'] for d in hourly_decisions if d['decision'] == 'CHARGE']\n",
    "        discharge_hours = [d['hour'] for d in hourly_decisions if d['decision'] == 'DISCHARGE']\n",
    "        \n",
    "        total_charged = sum(charge_schedule)\n",
    "        total_discharged = sum(discharge_schedule)\n",
    "        \n",
    "        avg_charge_price = np.mean([prices[h] for h in charge_hours]) if charge_hours else 0\n",
    "        avg_discharge_price = np.mean([prices[h] for h in discharge_hours]) if discharge_hours else 0\n",
    "        \n",
    "        summary_parts = []\n",
    "        \n",
    "        # Charging summary\n",
    "        if charge_hours:\n",
    "            summary_parts.append(\n",
    "                f\"Charging strategy: Charge during {len(charge_hours)} hours \"\n",
    "                f\"at average price {avg_charge_price:.2f} EUR/MWh. \"\n",
    "                f\"Total energy charged: {total_charged:.2f} kWh.\"\n",
    "            )\n",
    "        \n",
    "        # Discharging summary\n",
    "        if discharge_hours:\n",
    "            summary_parts.append(\n",
    "                f\"Discharging strategy: Discharge during {len(discharge_hours)} hours \"\n",
    "                f\"at average price {avg_discharge_price:.2f} EUR/MWh. \"\n",
    "                f\"Total energy discharged: {total_discharged:.2f} kWh.\"\n",
    "            )\n",
    "        \n",
    "        # Arbitrage analysis\n",
    "        if charge_hours and discharge_hours:\n",
    "            price_spread = avg_discharge_price - avg_charge_price\n",
    "            summary_parts.append(\n",
    "                f\"Price arbitrage: Buying at {avg_charge_price:.2f} and selling at {avg_discharge_price:.2f}, \"\n",
    "                f\"capturing a spread of {price_spread:.2f} EUR/MWh.\"\n",
    "            )\n",
    "        \n",
    "        # SOC management\n",
    "        min_soc = min(soc_trajectory)\n",
    "        max_soc = max(soc_trajectory)\n",
    "        soc_range = max_soc - min_soc\n",
    "        summary_parts.append(\n",
    "            f\"SOC management: Operates between {min_soc:.1%} and {max_soc:.1%} \"\n",
    "            f\"(utilizing {soc_range:.1%} of available capacity).\"\n",
    "        )\n",
    "        \n",
    "        return \" \".join(summary_parts)\n",
    "    \n",
    "    def _calculate_performance_metrics(\n",
    "        self,\n",
    "        charge_schedule: List[float],\n",
    "        discharge_schedule: List[float],\n",
    "        soc_trajectory: List[float],\n",
    "        prices: np.ndarray,\n",
    "        import_grid: List[float],\n",
    "        export_grid: List[float],\n",
    "        battery_params: BatteryParams\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various performance metrics\"\"\"\n",
    "        \n",
    "        total_charged_kwh = sum(charge_schedule)\n",
    "        total_discharged_kwh = sum(discharge_schedule)\n",
    "        \n",
    "        # Energy metrics\n",
    "        energy_throughput = total_charged_kwh + total_discharged_kwh\n",
    "        cycle_count = total_charged_kwh / battery_params.capacity_kwh if battery_params.capacity_kwh > 0 else 0\n",
    "        \n",
    "        # Financial metrics\n",
    "        charge_cost = sum(c * p / 1000 for c, p in zip(charge_schedule, prices))\n",
    "        discharge_revenue = sum(d * p / 1000 for d, p in zip(discharge_schedule, prices))\n",
    "        arbitrage_profit = discharge_revenue - charge_cost\n",
    "        \n",
    "        # Efficiency metrics\n",
    "        roundtrip_efficiency = (total_discharged_kwh / total_charged_kwh * 100) if total_charged_kwh > 0 else 0\n",
    "        theoretical_efficiency = battery_params.eta_c * battery_params.eta_d * 100\n",
    "        \n",
    "        # Utilization metrics\n",
    "        avg_soc = np.mean(soc_trajectory)\n",
    "        soc_range = max(soc_trajectory) - min(soc_trajectory)\n",
    "        capacity_utilization = soc_range * 100\n",
    "        \n",
    "        return {\n",
    "            \"total_energy_charged_kwh\": float(total_charged_kwh),\n",
    "            \"total_energy_discharged_kwh\": float(total_discharged_kwh),\n",
    "            \"net_energy_flow_kwh\": float(total_charged_kwh - total_discharged_kwh),\n",
    "            \"energy_throughput_kwh\": float(energy_throughput),\n",
    "            \"cycle_count\": float(cycle_count),\n",
    "            \"charging_cost_eur\": float(charge_cost),\n",
    "            \"discharging_revenue_eur\": float(discharge_revenue),\n",
    "            \"arbitrage_profit_eur\": float(arbitrage_profit),\n",
    "            \"realized_roundtrip_efficiency_percent\": float(roundtrip_efficiency),\n",
    "            \"theoretical_roundtrip_efficiency_percent\": float(theoretical_efficiency),\n",
    "            \"average_soc\": float(avg_soc),\n",
    "            \"soc_range_utilized\": float(soc_range),\n",
    "            \"capacity_utilization_percent\": float(capacity_utilization)\n",
    "        }\n",
    "    \n",
    "    def _validate_solution(\n",
    "        self,\n",
    "        charge_schedule: List[float],\n",
    "        discharge_schedule: List[float],\n",
    "        soc_trajectory: List[float],\n",
    "        battery_params: BatteryParams\n",
    "    ) -> bool:\n",
    "        \"\"\"Validate that solution satisfies all constraints\"\"\"\n",
    "        \n",
    "        # Check SOC bounds\n",
    "        soc_valid = all(\n",
    "            battery_params.soc_min - 1e-6 <= s <= battery_params.soc_max + 1e-6 \n",
    "            for s in soc_trajectory\n",
    "        )\n",
    "        \n",
    "        # Check power bounds\n",
    "        power_valid = all(\n",
    "            c <= battery_params.cmax_kw + 1e-6 and \n",
    "            d <= battery_params.dmax_kw + 1e-6\n",
    "            for c, d in zip(charge_schedule, discharge_schedule)\n",
    "        )\n",
    "        \n",
    "        # Check no simultaneous charge/discharge\n",
    "        no_simultaneous = all(\n",
    "            c < 1e-6 or d < 1e-6 \n",
    "            for c, d in zip(charge_schedule, discharge_schedule)\n",
    "        )\n",
    "        \n",
    "        return soc_valid and power_valid and no_simultaneous\n",
    "    \n",
    "    def _get_season(self, date_str: str) -> str:\n",
    "        \"\"\"Determine season from date\"\"\"\n",
    "        month = datetime.strptime(date_str, \"%Y-%m-%d\").month\n",
    "        if month in [12, 1, 2]:\n",
    "            return \"winter\"\n",
    "        elif month in [3, 4, 5]:\n",
    "            return \"spring\"\n",
    "        elif month in [6, 7, 8]:\n",
    "            return \"summer\"\n",
    "        else:\n",
    "            return \"fall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7011bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_for_region(\n",
    "    dataset_name: str,\n",
    "    split: str,\n",
    "    battery_params: BatteryParams,\n",
    "    data_dir: str = \"./data\",\n",
    "    output_dir: str = \"./datasets\",\n",
    "    max_days: int = None\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate fine-tuning dataset for a specific region and split\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: 'italy', 'germany', or 'caiso'\n",
    "        split: 'train' or 'test'\n",
    "        battery_params: Battery configuration\n",
    "        data_dir: Directory containing CSV files\n",
    "        output_dir: Directory to save output datasets\n",
    "        max_days: Maximum number of days to process (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        List of dataset entries\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GENERATING DATASET: {dataset_name.upper()} - {split.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    loader = MultiDatasetLoader(data_dir=data_dir)\n",
    "    df = loader.load_dataset(dataset_name, split)\n",
    "    daily_batches = loader.get_daily_batches(df)\n",
    "    \n",
    "    if max_days:\n",
    "        daily_batches = daily_batches[:max_days]\n",
    "        print(f\"Limiting to first {max_days} days\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = DatasetGenerator(battery_config={})\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = []\n",
    "    failures = []\n",
    "    \n",
    "    for idx, (date, prices, demand) in enumerate(daily_batches):\n",
    "        try:\n",
    "            # Create day inputs\n",
    "            day = DayInputs(\n",
    "                prices_buy=prices.tolist(),\n",
    "                demand_kw=demand.tolist(),\n",
    "                prices_sell=prices.tolist(),\n",
    "                allow_export=False,\n",
    "                dt_hours=1.0\n",
    "            )\n",
    "\n",
    "            milp_solution = solve_daily_milp(\n",
    "                batt=battery_params,\n",
    "                day=day,\n",
    "                solver=\"GUROBI\", \n",
    "                solver_opts={}\n",
    "            )\n",
    "            # Run MILP solver\n",
    "            # milp_solution = solve_daily_milp(\n",
    "            #     batt=battery_params,\n",
    "            #     day=day,\n",
    "            #     solver=None, \n",
    "            #     solver_opts={}\n",
    "            # )\n",
    "\n",
    "# # Run MILP solver\n",
    "# milp_solution = solve_daily_milp(\n",
    "#     batt=battery_params,\n",
    "#     day=day,\n",
    "#     solver=\"GUROBI\", \n",
    "#     solver_opts={}\n",
    "# )\n",
    "            \n",
    "            # Check if solution is valid\n",
    "            if milp_solution.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "                print(f\"  Warning: Day {date} - MILP status: {milp_solution.status}\")\n",
    "                failures.append({'date': date, 'reason': milp_solution.status})\n",
    "                continue\n",
    "            \n",
    "            # Create dataset entry\n",
    "            input_str = generator.create_input_component(\n",
    "                date=date,\n",
    "                prices=prices,\n",
    "                demand=demand,\n",
    "                battery_params=battery_params,\n",
    "                dataset_name=dataset_name\n",
    "            )\n",
    "            \n",
    "            output_str = generator.create_output_component(\n",
    "                milp_solution=milp_solution,\n",
    "                prices=prices,\n",
    "                demand=demand,\n",
    "                battery_params=battery_params\n",
    "            )\n",
    "            \n",
    "            dataset.append({\n",
    "                'instruction': generator.instruction,\n",
    "                'input': input_str,\n",
    "                'output': output_str\n",
    "            })\n",
    "            \n",
    "            # Update battery SOC for next day\n",
    "            if milp_solution.soc and len(milp_solution.soc) > 0:\n",
    "                battery_params.soc_init = milp_solution.soc[-1]\n",
    "            \n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(daily_batches)} days\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing day {date}: {e}\")\n",
    "            failures.append({'date': date, 'reason': str(e)})\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GENERATION COMPLETE: {dataset_name.upper()} - {split.upper()}\")\n",
    "    print(f\"  Successful: {len(dataset)} examples\")\n",
    "    print(f\"  Failed: {len(failures)} examples\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Save dataset\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    output_file = output_path / f\"{dataset_name}_{split}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset saved to: {output_file}\")\n",
    "    print(f\"Size: {len(json.dumps(dataset)) / 1024 / 1024:.2f} MB\\n\")\n",
    "    \n",
    "    # Save failures log if any\n",
    "    if failures:\n",
    "        failures_file = output_path / f\"{dataset_name}_{split}_failures.json\"\n",
    "        with open(failures_file, 'w') as f:\n",
    "            json.dump(failures, f, indent=2)\n",
    "        print(f\"Failures logged to: {failures_file}\\n\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def combine_datasets(\n",
    "    datasets: List[List[Dict]],\n",
    "    output_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine multiple datasets into one\n",
    "    \"\"\"\n",
    "    print(f\"\\nCombining {len(datasets)} datasets...\")\n",
    "    \n",
    "    combined = []\n",
    "    for ds in datasets:\n",
    "        combined.extend(ds)\n",
    "    \n",
    "    # Shuffle\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(combined)\n",
    "    \n",
    "    # Save\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(combined, f, indent=2)\n",
    "    \n",
    "    print(f\"Combined dataset saved to: {output_path}\")\n",
    "    print(f\"Total examples: {len(combined)}\")\n",
    "    print(f\"Size: {len(json.dumps(combined)) / 1024 / 1024:.2f} MB\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generate fine-tuning datasets from MILP solutions\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=['italy', 'germany', 'caiso', 'all'],\n",
    "        default='all',\n",
    "        help=\"Which dataset to generate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split\",\n",
    "        type=str,\n",
    "        choices=['train', 'test', 'both'],\n",
    "        default='both',\n",
    "        help=\"Which split to generate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"./data\",\n",
    "        help=\"Directory containing CSV files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        default=\"./datasets\",\n",
    "        help=\"Directory to save output datasets\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-days\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Maximum number of days per dataset (for testing)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generate-all\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Generate all datasets (all regions, train and test)\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Define battery configuration\n",
    "    # Adjust these based on your actual battery specs\n",
    "    battery_params = BatteryParams(\n",
    "        capacity_kwh=21.89,\n",
    "        cmax_kw=5.47,\n",
    "        dmax_kw=5.47,\n",
    "        eta_c=0.95,\n",
    "        eta_d=0.95,\n",
    "        soc_init=0.5,\n",
    "        soc_min=0.0,\n",
    "        soc_max=1.0,\n",
    "        soc_target=0.5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENERGY STORAGE DATASET GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nBattery Configuration:\")\n",
    "    print(f\"  Capacity: {battery_params.capacity_kwh} kWh\")\n",
    "    print(f\"  Max Charge: {battery_params.cmax_kw} kW\")\n",
    "    print(f\"  Max Discharge: {battery_params.dmax_kw} kW\")\n",
    "    print(f\"  Efficiency: {battery_params.eta_c}/{battery_params.eta_d}\")\n",
    "    print(f\"  SOC Range: [{battery_params.soc_min}, {battery_params.soc_max}]\")\n",
    "    \n",
    "    # Determine which datasets to generate\n",
    "    if args.generate_all or args.dataset == 'all':\n",
    "        datasets_to_generate = ['italy', 'germany', 'caiso']\n",
    "    else:\n",
    "        datasets_to_generate = [args.dataset]\n",
    "    \n",
    "    # Determine which splits to generate\n",
    "    if args.split == 'both':\n",
    "        splits_to_generate = ['train', 'test']\n",
    "    else:\n",
    "        splits_to_generate = [args.split]\n",
    "    \n",
    "    # Generate datasets\n",
    "    all_train_datasets = []\n",
    "    all_test_datasets = []\n",
    "    \n",
    "    for dataset_name in datasets_to_generate:\n",
    "        # Reset battery SOC for each region\n",
    "        battery_params.soc_init = 0.5\n",
    "        \n",
    "        for split in splits_to_generate:\n",
    "            dataset = generate_dataset_for_region(\n",
    "                dataset_name=dataset_name,\n",
    "                split=split,\n",
    "                battery_params=battery_params,\n",
    "                data_dir=args.data_dir,\n",
    "                output_dir=args.output_dir,\n",
    "                max_days=args.max_days\n",
    "            )\n",
    "            \n",
    "            if split == 'train':\n",
    "                all_train_datasets.append(dataset)\n",
    "            else:\n",
    "                all_test_datasets.append(dataset)\n",
    "    \n",
    "    # Combine all training datasets\n",
    "    if len(all_train_datasets) > 1:\n",
    "        combine_datasets(\n",
    "            all_train_datasets,\n",
    "            f\"{args.output_dir}/combined_train.json\"\n",
    "        )\n",
    "    \n",
    "    # Combine all test datasets\n",
    "    if len(all_test_datasets) > 1:\n",
    "        combine_datasets(\n",
    "            all_test_datasets,\n",
    "            f\"{args.output_dir}/combined_test.json\"\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET GENERATION PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Review generated datasets in:\", args.output_dir)\n",
    "    print(\"  2. Use combined_train.json for fine-tuning\")\n",
    "    print(\"  3. Use combined_test.json or individual test sets for evaluation\")\n",
    "    print(\"  4. Run: python train_with_unsloth.py --dataset ./datasets/combined_train.json\")\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6bfb580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING DATASET: ITALY - TRAIN\n",
      "================================================================================\n",
      "\n",
      "Loading italy data from agentic_energy\\data\\Italy_data.csv...\n",
      "  Loaded 8758 records from years [2022]\n",
      "  Date range: 2022-01-01 00:00:00 to 2022-12-31 23:00:00\n",
      "  Price range: 10.00 - 870.00\n",
      "  Demand range: 17.82 - 49.07\n",
      "  Warning: Skipping 2022-03-27 (only 22 hours)\n",
      "  Extracted 364 complete days\n",
      "Limiting to first 5 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16467\\OneDrive\\Desktop\\Columbia\\Agentics\\Another\\Agentics_for_EnergyArbitrage_Battery\\.venv\\Lib\\site-packages\\cvxpy\\expressions\\expression.py:683: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 6 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\16467\\OneDrive\\Desktop\\Columbia\\Agentics\\Another\\Agentics_for_EnergyArbitrage_Battery\\.venv\\Lib\\site-packages\\cvxpy\\expressions\\expression.py:683: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 7 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\16467\\OneDrive\\Desktop\\Columbia\\Agentics\\Another\\Agentics_for_EnergyArbitrage_Battery\\.venv\\Lib\\site-packages\\cvxpy\\expressions\\expression.py:683: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 8 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\16467\\OneDrive\\Desktop\\Columbia\\Agentics\\Another\\Agentics_for_EnergyArbitrage_Battery\\.venv\\Lib\\site-packages\\cvxpy\\expressions\\expression.py:683: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 9 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: ITALY - TRAIN\n",
      "  Successful: 5 examples\n",
      "  Failed: 0 examples\n",
      "================================================================================\n",
      "\n",
      "Dataset saved to: datasets\\italy_train.json\n",
      "Size: 0.11 MB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16467\\OneDrive\\Desktop\\Columbia\\Agentics\\Another\\Agentics_for_EnergyArbitrage_Battery\\.venv\\Lib\\site-packages\\cvxpy\\expressions\\expression.py:683: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 10 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from agentic_energy.schemas import BatteryParams\n",
    "\n",
    "battery_params = BatteryParams(\n",
    "    capacity_kwh=21.89,\n",
    "    cmax_kw=5.47,\n",
    "    dmax_kw=5.47,\n",
    "    eta_c=0.95,\n",
    "    eta_d=0.95,\n",
    "    soc_init=0.5,\n",
    "    soc_min=0.0,\n",
    "    soc_max=1.0,\n",
    "    soc_target=0.5\n",
    ")\n",
    "\n",
    "dataset = generate_dataset_for_region(\n",
    "    dataset_name='italy',\n",
    "    split='train',\n",
    "    battery_params=battery_params,\n",
    "    data_dir=\"./agentic_energy/data\",\n",
    "    output_dir=\"./datasets\",\n",
    "    max_days=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83de53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a2404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cebc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de1f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720d6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast Engine using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "# Import your existing framework components\n",
    "from agentic_energy.schemas import BatteryParams, DayInputs, SolveRequest, SolveResponse\n",
    "from agentic_energy.data_loader import BatteryDataLoader\n",
    "from agentic_energy.milp.milp_mcp_server import solve_daily_milp\n",
    "\n",
    "DATASET_CONFIG = {\n",
    "    'italy': {\n",
    "        'file': 'Italy_data.csv',\n",
    "        'train_years': [2022],\n",
    "        'test_years': [2023],\n",
    "        'price_column': 'prices',  # Adjust based on your CSV structure\n",
    "        'demand_column': 'consumption',  # Adjust based on your CSV structure\n",
    "        'timestamp_column': 'timestamps',  # Adjust based on your CSV structure\n",
    "    },\n",
    "    'germany': {\n",
    "        'file': 'Germany_energy_Data.csv',\n",
    "        'train_years': [2019, 2020, 2021, 2022],\n",
    "        'test_years': [2023],\n",
    "        'price_column': 'prices',\n",
    "        'demand_column': 'consumption',\n",
    "        'timestamp_column': 'timestamps',\n",
    "    },\n",
    "    'caiso': {\n",
    "        'file': 'CAISO_data.csv',\n",
    "        'train_years': [2021, 2022],\n",
    "        'test_years': [2023],\n",
    "        'price_column': 'prices',\n",
    "        'demand_column': 'consumption',\n",
    "        'timestamp_column': 'timestamps',\n",
    "    }\n",
    "}\n",
    "\n",
    "class MultiDatasetLoader:\n",
    "    \"\"\"Load and prepare data from multiple ISOs\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str = \"./data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "    def load_dataset(\n",
    "        self,\n",
    "        dataset_name: str,\n",
    "        split: str = 'train'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load dataset for specified split (train or test)\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: 'italy', 'germany', or 'caiso'\n",
    "            split: 'train' or 'test'\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with columns: timestamp, price, demand\n",
    "        \"\"\"\n",
    "        if dataset_name not in DATASET_CONFIG:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}. Use 'italy', 'germany', or 'caiso'\")\n",
    "        \n",
    "        config = DATASET_CONFIG[dataset_name]\n",
    "        file_path = self.data_dir / config['file']\n",
    "        \n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "        \n",
    "        # Load CSV\n",
    "        print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure timestamp is datetime\n",
    "        if config['timestamp_column'] in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df[config['timestamp_column']])\n",
    "        else:\n",
    "            raise ValueError(f\"Timestamp column '{config['timestamp_column']}' not found in {file_path}\")\n",
    "        \n",
    "        # Rename columns to standard names\n",
    "        df = df.rename(columns={\n",
    "            config['price_column']: 'price',\n",
    "            config['demand_column']: 'demand'\n",
    "        })\n",
    "        \n",
    "        # âœ… Handle European number format (comma as decimal separator)\n",
    "        # Convert to string first, then replace commas with periods\n",
    "        df['price'] = df['price'].astype(str).str.replace(',', '.', regex=False)\n",
    "        df['demand'] = df['demand'].astype(str).str.replace(',', '.', regex=False)\n",
    "        \n",
    "        # Now convert to numeric\n",
    "        df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "        df['demand'] = pd.to_numeric(df['demand'], errors='coerce')\n",
    "\n",
    "        if dataset_name == 'germany':\n",
    "            # original_mean = df['demand'].mean()\n",
    "            df['demand'] = df['demand'] / 1000\n",
    "            # print(f\"  âš™ï¸  Scaled Germany demand: {original_mean:.0f} MW â†’ {df['demand'].mean():.2f} MW\")\n",
    "    \n",
    "        \n",
    "        # Drop rows with missing values\n",
    "        rows_before = len(df)\n",
    "        df = df.dropna(subset=['price', 'demand'])\n",
    "        rows_after = len(df)\n",
    "        \n",
    "        if rows_before != rows_after:\n",
    "            print(f\"  âš ï¸  Dropped {rows_before - rows_after} rows with invalid/missing data\")\n",
    "        \n",
    "        # Extract year\n",
    "        df['year'] = df['timestamp'].dt.year\n",
    "        \n",
    "        # Filter by split\n",
    "        if split == 'train':\n",
    "            years = config['train_years']\n",
    "        elif split == 'test':\n",
    "            years = config['test_years']\n",
    "        else:\n",
    "            raise ValueError(f\"Split must be 'train' or 'test', got: {split}\")\n",
    "        \n",
    "        df_filtered = df[df['year'].isin(years)].copy()\n",
    "        \n",
    "        print(f\"  Loaded {len(df_filtered)} records from years {years}\")\n",
    "        print(f\"  Date range: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}\")\n",
    "        print(f\"  Price range: {df_filtered['price'].min():.2f} - {df_filtered['price'].max():.2f}\")\n",
    "        print(f\"  Demand range: {df_filtered['demand'].min():.2f} - {df_filtered['demand'].max():.2f}\")\n",
    "        \n",
    "        return df_filtered[['timestamp', 'price', 'demand', 'year']].sort_values('timestamp')\n",
    "    # def load_dataset(\n",
    "    #     self,\n",
    "    #     dataset_name: str,\n",
    "    #     split: str = 'train'\n",
    "    # ) -> pd.DataFrame:\n",
    "    #     \"\"\"\n",
    "    #     Load dataset for specified split (train or test)\n",
    "        \n",
    "    #     Args:\n",
    "    #         dataset_name: 'italy', 'germany', or 'caiso'\n",
    "    #         split: 'train' or 'test'\n",
    "            \n",
    "    #     Returns:\n",
    "    #         DataFrame with columns: timestamp, price, demand\n",
    "    #     \"\"\"\n",
    "    #     if dataset_name not in DATASET_CONFIG:\n",
    "    #         raise ValueError(f\"Unknown dataset: {dataset_name}. Use 'italy', 'germany', or 'caiso'\")\n",
    "        \n",
    "    #     config = DATASET_CONFIG[dataset_name]\n",
    "    #     file_path = self.data_dir / config['file']\n",
    "        \n",
    "    #     if not file_path.exists():\n",
    "    #         raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "        \n",
    "    #     # Load CSV\n",
    "    #     print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "    #     df = pd.read_csv(file_path)\n",
    "        \n",
    "    #     # Ensure timestamp is datetime\n",
    "    #     if config['timestamp_column'] in df.columns:\n",
    "    #         df['timestamp'] = pd.to_datetime(df[config['timestamp_column']])\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Timestamp column '{config['timestamp_column']}' not found in {file_path}\")\n",
    "        \n",
    "    #     # Rename columns to standard names\n",
    "    #     df = df.rename(columns={\n",
    "    #         config['price_column']: 'price',\n",
    "    #         config['demand_column']: 'demand'\n",
    "    #     })\n",
    "        \n",
    "    #     # Extract year\n",
    "    #     df['year'] = df['timestamp'].dt.year\n",
    "        \n",
    "    #     # Filter by split\n",
    "    #     if split == 'train':\n",
    "    #         years = config['train_years']\n",
    "    #     elif split == 'test':\n",
    "    #         years = config['test_years']\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Split must be 'train' or 'test', got: {split}\")\n",
    "        \n",
    "    #     df_filtered = df[df['year'].isin(years)].copy()\n",
    "        \n",
    "    #     print(f\"  Loaded {len(df_filtered)} records from years {years}\")\n",
    "    #     print(f\"  Date range: {df_filtered['timestamp'].min()} to {df_filtered['timestamp'].max()}\")\n",
    "    #     print(f\"  Price range: {df_filtered['price'].min():.2f} - {df_filtered['price'].max():.2f}\")\n",
    "    #     print(f\"  Demand range: {df_filtered['demand'].min():.2f} - {df_filtered['demand'].max():.2f}\")\n",
    "        \n",
    "    #     return df_filtered[['timestamp', 'price', 'demand', 'year']].sort_values('timestamp')\n",
    "    \n",
    "    def get_daily_batches(self, df: pd.DataFrame) -> List[Tuple[str, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Split dataframe into daily batches (24-hour periods)\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples: (date_str, prices_array, demand_array)\n",
    "        \"\"\"\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        \n",
    "        daily_batches = []\n",
    "        for date, group in df.groupby('date'):\n",
    "            if len(group) == 24:  # Only complete days\n",
    "                date_str = str(date)\n",
    "                prices = group['price'].values\n",
    "                demand = group['demand'].values\n",
    "                daily_batches.append((date_str, prices, demand))\n",
    "            else:\n",
    "                print(f\"  Warning: Skipping {date} (only {len(group)} hours)\")\n",
    "        \n",
    "        print(f\"  Extracted {len(daily_batches)} complete days\")\n",
    "        return daily_batches\n",
    "\n",
    "\n",
    "class DatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generates fine-tuning datasets from MILP solutions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, battery_config: Dict[str, float]):\n",
    "        self.battery_config = battery_config\n",
    "        self.instruction = self._create_instruction()\n",
    "        \n",
    "    def _create_instruction(self) -> str:\n",
    "        \"\"\"Create the static instruction component\"\"\"\n",
    "        return \"\"\"You are an energy storage optimization expert. Given electricity price forecasts and battery specifications, determine the optimal charge/discharge schedule for a 24-hour period to minimize operational costs while respecting all physical and operational constraints.\n",
    "\n",
    "Your task is to:\n",
    "1. Analyze the price forecast to identify charging opportunities (low prices) and discharging opportunities (high prices)\n",
    "2. Respect battery capacity (kWh), power limits (kW), efficiency losses, and state of charge (SOC) bounds\n",
    "3. Ensure SOC trajectory remains within [SOC_min, SOC_max] at all times\n",
    "4. Meet demand at every timestep\n",
    "5. Provide detailed reasoning for each hourly decision\n",
    "6. Output the complete schedule in JSON format\n",
    "\n",
    "Key decision principles:\n",
    "- Charge during hours with prices below average (especially if below 25th percentile)\n",
    "- Discharge during hours with prices above average (especially if above 75th percentile)\n",
    "- Account for round-trip efficiency losses (charge efficiency Ã— discharge efficiency)\n",
    "- Maintain feasible SOC trajectory throughout the day\n",
    "- Prioritize largest price arbitrage opportunities\n",
    "- Consider the time value of stored energy\n",
    "- Ensure smooth SOC transitions to avoid unnecessary cycling\n",
    "\n",
    "Output format:\n",
    "- Include complete charge/discharge schedule (kW for each hour)\n",
    "- Include SOC trajectory\n",
    "- Provide hourly decision reasoning\n",
    "- Calculate total operational cost\n",
    "- Validate all constraints are satisfied\"\"\"\n",
    "    \n",
    "    def create_input_component(\n",
    "        self,\n",
    "        date: str,\n",
    "        prices: np.ndarray,\n",
    "        demand: np.ndarray,\n",
    "        battery_params: BatteryParams,\n",
    "        dataset_name: str = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create the input component with all necessary context\n",
    "        \"\"\"\n",
    "        # Calculate price statistics\n",
    "        price_stats = {\n",
    "            \"min\": float(np.min(prices)),\n",
    "            \"max\": float(np.max(prices)),\n",
    "            \"mean\": float(np.mean(prices)),\n",
    "            \"median\": float(np.median(prices)),\n",
    "            \"std\": float(np.std(prices)),\n",
    "            \"p25\": float(np.percentile(prices, 25)),\n",
    "            \"p75\": float(np.percentile(prices, 75)),\n",
    "        }\n",
    "        \n",
    "        # Identify price patterns\n",
    "        peak_hours = [int(h) for h in range(24) if prices[h] > price_stats[\"p75\"]]\n",
    "        valley_hours = [int(h) for h in range(24) if prices[h] < price_stats[\"p25\"]]\n",
    "        \n",
    "        input_dict = {\n",
    "            \"date\": date,\n",
    "            \"region\": dataset_name.upper() if dataset_name else \"UNKNOWN\",\n",
    "            \"timestep\": \"hourly\",\n",
    "            \"horizon_hours\": 24,\n",
    "            \n",
    "            # Price data\n",
    "            \"price_forecast_eur_per_mwh\": prices.tolist(),\n",
    "            \"price_statistics\": price_stats,\n",
    "            \"price_patterns\": {\n",
    "                \"peak_hours\": peak_hours,\n",
    "                \"valley_hours\": valley_hours,\n",
    "                \"price_range\": float(price_stats[\"max\"] - price_stats[\"min\"]),\n",
    "                \"volatility_coefficient\": float(price_stats[\"std\"] / price_stats[\"mean\"]) if price_stats[\"mean\"] > 0 else 0.0\n",
    "            },\n",
    "            \n",
    "            # Demand data\n",
    "            \"demand_forecast_kw\": demand.tolist(),\n",
    "            \"demand_statistics\": {\n",
    "                \"min\": float(np.min(demand)),\n",
    "                \"max\": float(np.max(demand)),\n",
    "                \"mean\": float(np.mean(demand)),\n",
    "                \"total_daily_kwh\": float(np.sum(demand))\n",
    "            },\n",
    "            \n",
    "            # Battery specifications\n",
    "            \"battery_specifications\": {\n",
    "                \"capacity_kwh\": float(battery_params.capacity_kwh),\n",
    "                \"max_charge_power_kw\": float(battery_params.cmax_kw),\n",
    "                \"max_discharge_power_kw\": float(battery_params.dmax_kw),\n",
    "                \"charge_efficiency\": float(battery_params.eta_c),\n",
    "                \"discharge_efficiency\": float(battery_params.eta_d),\n",
    "                \"roundtrip_efficiency\": float(battery_params.eta_c * battery_params.eta_d),\n",
    "                \"initial_soc\": float(battery_params.soc_init),\n",
    "                \"soc_minimum\": float(battery_params.soc_min),\n",
    "                \"soc_maximum\": float(battery_params.soc_max),\n",
    "                \"soc_target_end_of_day\": float(battery_params.soc_target) if battery_params.soc_target is not None else None,\n",
    "            },\n",
    "            \n",
    "            # Operational constraints\n",
    "            \"operational_constraints\": {\n",
    "                \"timestep_duration_hours\": 1.0,\n",
    "                \"allow_grid_export\": True,\n",
    "                \"must_meet_demand_every_hour\": True,\n",
    "                \"simultaneous_charge_discharge\": False\n",
    "            },\n",
    "            \n",
    "            # Market context\n",
    "            \"market_context\": {\n",
    "                \"day_of_week\": datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%A\"),\n",
    "                \"season\": self._get_season(date),\n",
    "                \"expected_arbitrage_opportunities\": len(peak_hours) * len(valley_hours) > 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return json.dumps(input_dict, indent=2)\n",
    "    \n",
    "    def create_output_component(\n",
    "        self,\n",
    "        milp_solution: SolveResponse,\n",
    "        prices: np.ndarray,\n",
    "        demand: np.ndarray,\n",
    "        battery_params: BatteryParams\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create the output component from MILP solution with detailed reasoning\n",
    "        \"\"\"\n",
    "        # Extract MILP solution components\n",
    "        charge_schedule = milp_solution.charge_kw if milp_solution.charge_kw else [0] * 24\n",
    "        discharge_schedule = milp_solution.discharge_kw if milp_solution.discharge_kw else [0] * 24\n",
    "        soc_trajectory = milp_solution.soc if milp_solution.soc else [battery_params.soc_init] * 25\n",
    "        import_grid = milp_solution.import_kw if milp_solution.import_kw else demand.tolist()\n",
    "        export_grid = milp_solution.export_kw if milp_solution.export_kw else [0] * 24\n",
    "        total_cost = milp_solution.objective_cost if milp_solution.objective_cost else 0.0\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_price = np.mean(prices)\n",
    "        p25_price = np.percentile(prices, 25)\n",
    "        p75_price = np.percentile(prices, 75)\n",
    "        \n",
    "        # Generate hourly reasoning\n",
    "        hourly_decisions = []\n",
    "        for hour in range(24):\n",
    "            decision = self._analyze_hour_decision(\n",
    "                hour=hour,\n",
    "                price=prices[hour],\n",
    "                charge=charge_schedule[hour],\n",
    "                discharge=discharge_schedule[hour],\n",
    "                soc_before=soc_trajectory[hour],\n",
    "                soc_after=soc_trajectory[hour + 1] if hour + 1 < len(soc_trajectory) else soc_trajectory[hour],\n",
    "                avg_price=avg_price,\n",
    "                p25_price=p25_price,\n",
    "                p75_price=p75_price,\n",
    "                battery_params=battery_params,\n",
    "                demand=demand[hour]\n",
    "            )\n",
    "            hourly_decisions.append(decision)\n",
    "        \n",
    "        # Generate strategy summary\n",
    "        strategy_summary = self._generate_strategy_summary(\n",
    "            hourly_decisions, prices, charge_schedule, discharge_schedule, soc_trajectory\n",
    "        )\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        performance_metrics = self._calculate_performance_metrics(\n",
    "            charge_schedule, discharge_schedule, soc_trajectory,\n",
    "            prices, import_grid, export_grid, battery_params\n",
    "        )\n",
    "        \n",
    "        # Construct output\n",
    "        output_dict = {\n",
    "            \"optimization_status\": milp_solution.status if milp_solution.status else \"unknown\",\n",
    "            \"objective_value\": {\n",
    "                \"total_cost_eur\": float(total_cost),\n",
    "                \"average_hourly_cost_eur\": float(total_cost / 24) if total_cost else 0.0\n",
    "            },\n",
    "            \n",
    "            \"schedule\": {\n",
    "                \"charge_kw\": [float(c) for c in charge_schedule],\n",
    "                \"discharge_kw\": [float(d) for d in discharge_schedule],\n",
    "                \"soc_trajectory\": [float(s) for s in soc_trajectory],\n",
    "                \"grid_import_kw\": [float(i) for i in import_grid],\n",
    "                \"grid_export_kw\": [float(e) for e in export_grid]\n",
    "            },\n",
    "            \n",
    "            \"strategy_summary\": strategy_summary,\n",
    "            \n",
    "            \"hourly_decision_reasoning\": hourly_decisions,\n",
    "            \n",
    "            \"performance_metrics\": performance_metrics,\n",
    "            \n",
    "            \"validation\": {\n",
    "                \"all_constraints_satisfied\": self._validate_solution(\n",
    "                    charge_schedule, discharge_schedule, soc_trajectory, battery_params\n",
    "                ),\n",
    "                \"min_soc_value\": float(np.min(soc_trajectory)),\n",
    "                \"max_soc_value\": float(np.max(soc_trajectory)),\n",
    "                \"soc_violations\": int(np.sum([\n",
    "                    1 for s in soc_trajectory \n",
    "                    if s < battery_params.soc_min - 1e-6 or s > battery_params.soc_max + 1e-6\n",
    "                ])),\n",
    "                \"power_violations\": int(np.sum([\n",
    "                    1 for c, d in zip(charge_schedule, discharge_schedule)\n",
    "                    if c > battery_params.cmax_kw + 1e-6 or d > battery_params.dmax_kw + 1e-6\n",
    "                ]))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return json.dumps(output_dict, indent=2)\n",
    "    \n",
    "    def _analyze_hour_decision(\n",
    "        self,\n",
    "        hour: int,\n",
    "        price: float,\n",
    "        charge: float,\n",
    "        discharge: float,\n",
    "        soc_before: float,\n",
    "        soc_after: float,\n",
    "        avg_price: float,\n",
    "        p25_price: float,\n",
    "        p75_price: float,\n",
    "        battery_params: BatteryParams,\n",
    "        demand: float\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Generate detailed reasoning for a single hour's decision\"\"\"\n",
    "        \n",
    "        # Determine decision type\n",
    "        if charge > 0.01:\n",
    "            decision_type = \"CHARGE\"\n",
    "            action_magnitude = charge\n",
    "        elif discharge > 0.01:\n",
    "            decision_type = \"DISCHARGE\"\n",
    "            action_magnitude = discharge\n",
    "        else:\n",
    "            decision_type = \"IDLE\"\n",
    "            action_magnitude = 0.0\n",
    "        \n",
    "        # Generate reasoning\n",
    "        reasoning_parts = []\n",
    "        \n",
    "        # Price analysis\n",
    "        if decision_type == \"CHARGE\":\n",
    "            if price < p25_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is in the bottom quartile (below {p25_price:.2f}), \"\n",
    "                    f\"making this an excellent charging opportunity.\"\n",
    "                )\n",
    "            elif price < avg_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is below average ({avg_price:.2f}), \"\n",
    "                    f\"making this a good charging window.\"\n",
    "                )\n",
    "\n",
    "            # reasoning misses the correct way to analyze, can you scale the \n",
    "            reasoning_parts.append(\n",
    "                f\"Charging at {charge:.2f} kW to store energy for later use during more expensive hours.\"\n",
    "            )\n",
    "            \n",
    "        elif decision_type == \"DISCHARGE\":\n",
    "            if price > p75_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is in the top quartile (above {p75_price:.2f}), \"\n",
    "                    f\"making this an excellent discharging opportunity.\"\n",
    "                )\n",
    "            elif price > avg_price:\n",
    "                reasoning_parts.append(\n",
    "                    f\"Price {price:.2f} EUR/MWh is above average ({avg_price:.2f}), \"\n",
    "                    f\"making this a good discharging window.\"\n",
    "                )\n",
    "            reasoning_parts.append(\n",
    "                f\"Discharging at {discharge:.2f} kW to reduce grid imports during expensive hours.\"\n",
    "            )\n",
    "            \n",
    "        else:  # IDLE\n",
    "            price_position = \"near average\" if abs(price - avg_price) < 0.1 * avg_price else \"moderate\"\n",
    "            reasoning_parts.append(\n",
    "                f\"Price {price:.2f} EUR/MWh is {price_position} ({avg_price:.2f} average), \"\n",
    "                f\"not offering sufficient arbitrage opportunity to justify battery action.\"\n",
    "            )\n",
    "        \n",
    "        # SOC constraints\n",
    "        soc_change = soc_after - soc_before\n",
    "        if abs(soc_change) > 0.001:\n",
    "            reasoning_parts.append(\n",
    "                f\"SOC changes from {soc_before:.1%} to {soc_after:.1%} \"\n",
    "                f\"({'increasing' if soc_change > 0 else 'decreasing'} by {abs(soc_change):.1%}).\"\n",
    "            )\n",
    "        \n",
    "        # Constraint boundaries\n",
    "        if soc_after >= battery_params.soc_max - 0.01:\n",
    "            reasoning_parts.append(\"Battery reaches maximum SOC capacity.\")\n",
    "        elif soc_after <= battery_params.soc_min + 0.01:\n",
    "            reasoning_parts.append(\"Battery reaches minimum SOC limit.\")\n",
    "        \n",
    "        if decision_type == \"CHARGE\" and charge >= battery_params.cmax_kw - 0.01:\n",
    "            reasoning_parts.append(\"Charging at maximum power capacity.\")\n",
    "        elif decision_type == \"DISCHARGE\" and discharge >= battery_params.dmax_kw - 0.01:\n",
    "            reasoning_parts.append(\"Discharging at maximum power capacity.\")\n",
    "        \n",
    "        # Energy balance\n",
    "        net_demand = demand - discharge + charge\n",
    "        reasoning_parts.append(f\"Net grid import: {net_demand:.2f} kW to meet {demand:.2f} kW demand.\")\n",
    "        \n",
    "        return {\n",
    "            \"hour\": hour,\n",
    "            \"time_of_day\": f\"{hour:02d}:00\",\n",
    "            \"electricity_price_eur_per_mwh\": float(price),\n",
    "            \"price_vs_average\": float(price - avg_price),\n",
    "            \"decision\": decision_type,\n",
    "            \"charge_power_kw\": float(charge),\n",
    "            \"discharge_power_kw\": float(discharge),\n",
    "            \"soc_before_action\": float(soc_before),\n",
    "            \"soc_after_action\": float(soc_after),\n",
    "            \"soc_change\": float(soc_change),\n",
    "            \"detailed_reasoning\": \" \".join(reasoning_parts)\n",
    "        }\n",
    "    \n",
    "    def _generate_strategy_summary(\n",
    "        self,\n",
    "        hourly_decisions: List[Dict],\n",
    "        prices: np.ndarray,                         # EUR/MWh\n",
    "        charge_schedule: List[float],               # kWh/interval\n",
    "        discharge_schedule: List[float],            # kWh/interval\n",
    "        soc_trajectory: List[float],                # fraction in [0,1]\n",
    "        *,\n",
    "        # physics/econ\n",
    "        eta_c: float = 0.95,\n",
    "        eta_d: float = 0.95,\n",
    "        var_om_eur_per_mwh: float = 0.0,\n",
    "        schedule_side: str = \"ac\",                  # \"ac\" or \"dc\"\n",
    "        dt_hours: float = 1.0,\n",
    "        capacity_kwh: Optional[float] = None,\n",
    "        p_max_kw: Optional[float] = None,\n",
    "        soc_floor: float = 0.0,\n",
    "        soc_ceiling: float = 1.0,\n",
    "        # analysis windows\n",
    "        kelly_window: int = 12,\n",
    "        spread_window_future: int = 6,\n",
    "        spread_window_past: int = 6,\n",
    "        # output style\n",
    "        verbose: bool = True,\n",
    "        eps: float = 1e-6,\n",
    "    ) -> str:\n",
    "        import numpy as np\n",
    "\n",
    "        T = len(prices)\n",
    "        p = np.asarray(prices, float)\n",
    "        ch = np.asarray(charge_schedule, float)\n",
    "        dis = np.asarray(discharge_schedule, float)\n",
    "        soc = np.asarray(soc_trajectory, float)\n",
    "        if len(soc) == T:\n",
    "            soc = np.concatenate([soc, soc[-1:]])\n",
    "\n",
    "        # decisions â†’ hour sets (fallback to schedules)\n",
    "        charge_hours = [d['hour'] for d in hourly_decisions if d.get('decision') == 'CHARGE'] \\\n",
    "                    if hourly_decisions else list(np.where(ch > eps)[0])\n",
    "        discharge_hours = [d['hour'] for d in hourly_decisions if d.get('decision') == 'DISCHARGE'] \\\n",
    "                        if hourly_decisions else list(np.where(dis > eps)[0])\n",
    "\n",
    "        # Market-side energy for economics\n",
    "        if schedule_side.lower() == \"dc\":\n",
    "            grid_buy_mwh  = (ch / max(eta_c, 1e-9)) / 1000.0\n",
    "            grid_sell_mwh = (dis * eta_d) / 1000.0\n",
    "        else:\n",
    "            grid_buy_mwh  = ch / 1000.0\n",
    "            grid_sell_mwh = dis / 1000.0\n",
    "\n",
    "        # Totals & prices\n",
    "        total_charged_kwh = float(ch.sum())\n",
    "        total_discharged_kwh = float(dis.sum())\n",
    "        idle_hours = int(np.sum((ch <= eps) & (dis <= eps)))\n",
    "        avg_charge_price = float(np.mean(p[charge_hours])) if charge_hours else 0.0\n",
    "        avg_discharge_price = float(np.mean(p[discharge_hours])) if discharge_hours else 0.0\n",
    "\n",
    "        # Economics\n",
    "        revenue_eur = float(np.sum(p * grid_sell_mwh))\n",
    "        energy_cost_eur = float(np.sum(p * grid_buy_mwh))\n",
    "        throughput_mwh = float(grid_buy_mwh.sum() + grid_sell_mwh.sum())\n",
    "        vom_eur = var_om_eur_per_mwh * throughput_mwh\n",
    "        gross_profit_eur = revenue_eur - energy_cost_eur\n",
    "        net_profit_eur = gross_profit_eur - vom_eur\n",
    "\n",
    "        # Spread tests (temporal arbitrage condition)\n",
    "        eta_rt = eta_c * eta_d\n",
    "        spread_realized = (avg_discharge_price - avg_charge_price) if (charge_hours and discharge_hours) else 0.0\n",
    "        spread_required = (\n",
    "            avg_charge_price * (1.0/max(eta_rt,1e-9) - 1.0) + var_om_eur_per_mwh / max(eta_rt, 1e-9)\n",
    "        ) if charge_hours else 0.0\n",
    "        spread_margin = spread_realized - spread_required if (charge_hours and discharge_hours) else 0.0\n",
    "        arbitrage_profitable = (spread_margin > 0.0)\n",
    "\n",
    "        # Quantile diagnostics\n",
    "        q25, q50, q75 = np.quantile(p, [0.25, 0.50, 0.75])\n",
    "        frac_charge_below_q25 = (np.mean(p[charge_hours] <= q25) if charge_hours else 0.0)\n",
    "        frac_discharge_above_q75 = (np.mean(p[discharge_hours] >= q75) if discharge_hours else 0.0)\n",
    "\n",
    "        # Streaks\n",
    "        def longest_streak(mask):\n",
    "            best = cur = 0\n",
    "            for v in mask:\n",
    "                cur = cur + 1 if v else 0\n",
    "                best = max(best, cur)\n",
    "            return best\n",
    "        longest_charge_streak = longest_streak(ch > eps)\n",
    "        longest_discharge_streak = longest_streak(dis > eps)\n",
    "\n",
    "        # Bindings/utilization\n",
    "        at_floor = int(np.sum(soc[:-1] <= (soc_floor + 1e-9)))\n",
    "        at_ceiling = int(np.sum(soc[:-1] >= (soc_ceiling - 1e-9)))\n",
    "        charge_at_pmax = discharge_at_pmax = 0\n",
    "        if p_max_kw is not None and dt_hours > 0:\n",
    "            charge_at_pmax = int(np.sum(ch >= (0.99 * p_max_kw * dt_hours)))\n",
    "            discharge_at_pmax = int(np.sum(dis >= (0.99 * p_max_kw * dt_hours)))\n",
    "        time_utilization = 1.0 - (idle_hours / max(T,1))\n",
    "        energy_utilization = (\n",
    "            (total_charged_kwh + total_discharged_kwh) / (T * (p_max_kw * dt_hours))\n",
    "            if (p_max_kw is not None and p_max_kw > 0) else None\n",
    "        )\n",
    "        fce = (\n",
    "            (total_charged_kwh + total_discharged_kwh) / (2.0 * capacity_kwh)\n",
    "            if (capacity_kwh is not None and capacity_kwh > 0) else None\n",
    "        )\n",
    "\n",
    "        # SoC band\n",
    "        min_soc, max_soc = float(np.min(soc)), float(np.max(soc))\n",
    "        soc_range = max_soc - min_soc\n",
    "        soc_drift = float(soc[-1] - soc[0])\n",
    "\n",
    "        # Arbitrage indicator & rolling spread windows\n",
    "        dp = np.diff(p, prepend=p[0])\n",
    "        p_avg = p.mean()\n",
    "        p_std = p.std(ddof=0) if p.std(ddof=0) > 0 else 1.0\n",
    "        A = (p < (p_avg - p_std)).astype(int) - (p > (p_avg + p_std)).astype(int)    # +1 charge, -1 discharge, 0 idle\n",
    "        frac_A_pos = float(np.mean(A == 1.0))\n",
    "        frac_A_neg = float(np.mean(A == -1.0))\n",
    "        act = np.zeros(T, dtype=int); act[ch > eps] = 1; act[dis > eps] = -1\n",
    "        indicator_alignment = float(np.mean(A == act)) if T > 0 else 0.0\n",
    "\n",
    "        fw = spread_window_future\n",
    "        bw = spread_window_past\n",
    "        future_max = np.array([p[t+1:t+1+fw].max() if t+1 < T else p[t] for t in range(T)])\n",
    "        past_min   = np.array([p[max(0, t-bw):t].min() if t > 0 else p[t] for t in range(T)])\n",
    "        spread_future = future_max - p\n",
    "        spread_past   = p - past_min\n",
    "        feas_charge = int(np.sum(spread_future > spread_required))\n",
    "        feas_discharge = int(np.sum(spread_past   > spread_required))\n",
    "\n",
    "        # Arbitrage Index\n",
    "        denom = float(np.sum(np.abs(dp))) or 1.0\n",
    "        arbitrage_index = float(np.sum(np.maximum(0.0, dp)) / denom)\n",
    "\n",
    "        # Kelly proxy\n",
    "        r = np.zeros(T); r[1:] = (p[1:] - p[:-1]) / np.maximum(p[:-1], 1e-12)\n",
    "        mu = np.array([np.mean(r[max(0,t-kelly_window+1):t+1]) for t in range(T)])\n",
    "        var = np.array([np.var (r[max(0,t-kelly_window+1):t+1], ddof=0) for t in range(T)])\n",
    "        var[var < 1e-12] = 1e-12\n",
    "        kelly_f = np.clip(mu / var, -1.0, 1.0)\n",
    "        ksign = np.sign(kelly_f)\n",
    "        nonzero = (act != 0)\n",
    "        kelly_alignment = float(np.mean(ksign[nonzero] == act[nonzero])) if np.any(nonzero) else 0.0\n",
    "        avg_kelly_mag_charge = float(np.mean(np.abs(kelly_f[act == 1]))) if np.any(act == 1) else 0.0\n",
    "        avg_kelly_mag_discharge = float(np.mean(np.abs(kelly_f[act == -1]))) if np.any(act == -1) else 0.0\n",
    "\n",
    "        # ---------- Formatting helpers ----------\n",
    "        pct = lambda x: f\"{100.0 * x:.1f}%\"\n",
    "        eur = lambda x: f\"{x:,.2f} â‚¬\"\n",
    "        mwh = lambda x: f\"{x:.3f} MWh\"\n",
    "        kwh = lambda x: f\"{x:.1f} kWh\"\n",
    "\n",
    "        parts = []\n",
    "\n",
    "        if verbose:\n",
    "            # Charging paragraph\n",
    "            if charge_hours:\n",
    "                parts.append(\n",
    "                    (\n",
    "                        f\"The strategy charges the battery during {len(charge_hours)} distinct hours when market prices \"\n",
    "                        f\"tend to be relatively low. The average purchase price paid during charging is \"\n",
    "                        f\"{avg_charge_price:.2f} â‚¬/MWh. Notably, {pct(frac_charge_below_q25)} of all charging activity occurs \"\n",
    "                        f\"when the price falls at or below the 25th percentile of the entire price distribution, indicating a \"\n",
    "                        f\"systematic preference for the cheapest periods. In total, the battery buys {kwh(total_charged_kwh)} \"\n",
    "                        f\"of energy from the grid over the horizon.\"\n",
    "                    )\n",
    "                )\n",
    "            # Discharging paragraph\n",
    "            if discharge_hours:\n",
    "                parts.append(\n",
    "                    (\n",
    "                        f\"The strategy discharges during {len(discharge_hours)} hours that coincide with elevated prices. \"\n",
    "                        f\"The average selling price realized during discharging is {avg_discharge_price:.2f} â‚¬/MWh. \"\n",
    "                        f\"Furthermore, {pct(frac_discharge_above_q75)} of discharging occurs when the price is at or above the \"\n",
    "                        f\"75th percentile, which confirms that the policy concentrates sales in the most lucrative windows. \"\n",
    "                        f\"Across the horizon, the system sells {kwh(total_discharged_kwh)} of energy back to the grid.\"\n",
    "                    )\n",
    "                )\n",
    "            # Arbitrage economics paragraph\n",
    "            if charge_hours and discharge_hours:\n",
    "                profitability_text = (\n",
    "                    \"This realized spread exceeds the minimum required spread once round-trip efficiency losses and variable \"\n",
    "                    \"O&M are accounted for, which implies that the temporal arbitrage executed by the policy is profitable on average.\"\n",
    "                    if arbitrage_profitable else\n",
    "                    \"This realized spread does not meet the minimum required threshold after accounting for round-trip efficiency \"\n",
    "                    \"losses and variable O&M, suggesting that the temporal arbitrage is not profitable on average.\"\n",
    "                )\n",
    "                parts.append(\n",
    "                    (\n",
    "                        f\"From a temporal arbitrage perspective, the policy buys low and sells high: the realized average spread \"\n",
    "                        f\"between selling and buying prices is {spread_realized:.2f} â‚¬/MWh. Based on a round-trip efficiency of \"\n",
    "                        f\"{eta_rt:.3f} and a variable O&M estimate of {var_om_eur_per_mwh:.2f} â‚¬/MWh, the required break-even spread \"\n",
    "                        f\"is {spread_required:.2f} â‚¬/MWh. The resulting margin is {spread_margin:.2f} â‚¬/MWh. {profitability_text}\"\n",
    "                    )\n",
    "                )\n",
    "            # P&L paragraph\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"Economic results reflect these mechanics: total market revenue from sales is {eur(revenue_eur)}, while \"\n",
    "                    f\"the cost of energy purchased is {eur(energy_cost_eur)}. On a throughput of {mwh(throughput_mwh)}, the variable \"\n",
    "                    f\"O&M cost amounts to {eur(vom_eur)}. Combining these terms yields a gross profit of {eur(gross_profit_eur)} and \"\n",
    "                    f\"a net profit of {eur(net_profit_eur)} over the analyzed period.\"\n",
    "                )\n",
    "            )\n",
    "            # SoC management paragraph\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"The state of charge (SoC) operates within a band from {100*min_soc:.1f}% to {100*max_soc:.1f}% \"\n",
    "                    f\"(a utilization range of {pct(soc_range)}). Over the full horizon, the SoC exhibits a net drift of \"\n",
    "                    f\"{pct(soc_drift)}, indicating the overall tendency to end with a higher or lower inventory relative to the start. \"\n",
    "                    f\"The trajectory reaches the SoC floor on {at_floor} hour(s) and the SoC ceiling on {at_ceiling} hour(s), which \"\n",
    "                    f\"highlights how often the energy capacity constraints are binding. The asset remains idle for {idle_hours} hour(s), \"\n",
    "                    f\"and the longest continuous charging and discharging streaks are {longest_charge_streak} and \"\n",
    "                    f\"{longest_discharge_streak} hour(s), respectively.\"\n",
    "                )\n",
    "            )\n",
    "            # Power/Utilization/Cycles paragraph\n",
    "            if p_max_kw is not None:\n",
    "                hit_text = (\n",
    "                    f\"Charging power hits its upper limit during {charge_at_pmax} hour(s) and discharging power hits its \"\n",
    "                    f\"upper limit during {discharge_at_pmax} hour(s), suggesting frequent operation at nameplate constraints.\"\n",
    "                )\n",
    "            else:\n",
    "                hit_text = \"Power limit diagnostics were not evaluated because a nameplate limit was not provided.\"\n",
    "            util_text = (\n",
    "                f\"The time-based utilization is {pct(time_utilization)} of the horizon. \"\n",
    "                f\"{('Energy utilization relative to the power limit is ' + pct(energy_utilization) + '. ') if energy_utilization is not None else ''}\"\n",
    "                f\"{('Equivalent full cycles are estimated at ' + f'{fce:.2f}' + ' over the period, which contextualizes throughput vs. capacity.') if fce is not None else ''}\"\n",
    "            )\n",
    "            parts.append(hit_text + \" \" + util_text)\n",
    "\n",
    "            # Indicator paragraph\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"To interpret action timing independently of the optimizer, we compute an arbitrage indicator \"\n",
    "                    f\"A_t = 1{{p_t < Î¼_p âˆ’ Ïƒ_p}} âˆ’ 1{{p_t > Î¼_p + Ïƒ_p}}, where Î¼_p and Ïƒ_p are the sample mean and standard deviation of price. \"\n",
    "                    f\"According to this heuristic, {pct(frac_A_pos)} of intervals suggested charging and {pct(frac_A_neg)} suggested discharging. \"\n",
    "                    f\"The realized actions agree with this indicator in {pct(indicator_alignment)} of intervals, indicating the degree to which the \"\n",
    "                    f\"policy follows a simple buy-low/sell-high rule of thumb.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Rolling spread window paragraph\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"We also examine rolling spread windows to quantify actionable opportunities. For each interval, the forward-looking \"\n",
    "                    f\"{spread_window_future}-hour window identifies the best future price relative to the current price, yielding a potential \"\n",
    "                    f\"charge-side spread; symmetrically, the backward-looking {spread_window_past}-hour window compares the current price with the \"\n",
    "                    f\"minimum of recent prices, yielding a discharge-side spread. Counting only windows whose spread exceeds the break-even requirement \"\n",
    "                    f\"({spread_required:.2f} â‚¬/MWh), we find {feas_charge} feasible charge opportunities and {feas_discharge} feasible \"\n",
    "                    f\"discharge opportunities. These counts reflect how often the market provides economically meaningful temporal arbitrage.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Arbitrage Index paragraph\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"As a summary measure of directional structure versus oscillation in the price series, the Arbitrage Index is \"\n",
    "                    f\"AI = Î£ max(0, Î”p_t) / Î£ |Î”p_t| = {arbitrage_index:.2f}. Values near 0.5 indicate a balanced, mean-reverting profile \"\n",
    "                    f\"that is typically favorable for short-horizon arbitrage; values approaching 1 indicate persistent uptrends (fewer buy-low \"\n",
    "                    f\"opportunities), while values approaching 0 indicate persistent downtrends (fewer sell-high opportunities).\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Kelly proxy paragraph\n",
    "            parts.append(\n",
    "                (\n",
    "                    f\"Finally, we compare actions to a Kelly-style control signal derived from rolling price returns. Using a window of {kelly_window} \"\n",
    "                    f\"periods, we estimate the expected return and variance and form a bounded Kelly fraction f*_t â‰ˆ Î¼_t / Ïƒ_tÂ². The sign of this fraction \"\n",
    "                    f\"indicates whether charging (positive) or discharging (negative) would maximize expected log-growth, while its magnitude reflects \"\n",
    "                    f\"the suggested aggressiveness. Over intervals with non-zero actions, the sign of the Kelly proxy matches realized actions in \"\n",
    "                    f\"{pct(kelly_alignment)} of cases. Conditional on action, the average recommended aggressiveness is {avg_kelly_mag_charge:.2f} \"\n",
    "                    f\"while charging and {avg_kelly_mag_discharge:.2f} while discharging, providing a risk-aware benchmark for allocation sizing.\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Concise mode (previous short form)\n",
    "            if charge_hours:\n",
    "                parts.append(\n",
    "                    f\"Charging: {len(charge_hours)} h @ {avg_charge_price:.2f} â‚¬/MWh, {pct(frac_charge_below_q25)} â‰¤ Q25, total {kwh(total_charged_kwh)}.\"\n",
    "                )\n",
    "            if discharge_hours:\n",
    "                parts.append(\n",
    "                    f\"Discharging: {len(discharge_hours)} h @ {avg_discharge_price:.2f} â‚¬/MWh, {pct(frac_discharge_above_q75)} â‰¥ Q75, total {kwh(total_discharged_kwh)}.\"\n",
    "                )\n",
    "            if charge_hours and discharge_hours:\n",
    "                parts.append(\n",
    "                    f\"Arbitrage: spread {spread_realized:.2f} vs req {spread_required:.2f} (Î·_rt={eta_rt:.3f}, VOM={var_om_eur_per_mwh:.2f}); margin {spread_margin:.2f} â†’ \"\n",
    "                    f\"{'profitable' if arbitrage_profitable else 'not profitable'}.\"\n",
    "                )\n",
    "            parts.append(\n",
    "                f\"Economics: revenue {eur(revenue_eur)}, energy cost {eur(energy_cost_eur)}, VOM {eur(vom_eur)} on {mwh(throughput_mwh)} â†’ net {eur(net_profit_eur)}.\"\n",
    "            )\n",
    "            parts.append(\n",
    "                f\"SoC: {100*min_soc:.1f}%â€“{100*max_soc:.1f}% (range {pct(soc_range)}), drift {pct(soc_drift)}; floor {at_floor} h, ceiling {at_ceiling} h; idle {idle_hours} h.\"\n",
    "            )\n",
    "            if p_max_kw is not None:\n",
    "                parts.append(f\"Power limits hit: charge {charge_at_pmax} h, discharge {discharge_at_pmax} h.\")\n",
    "            parts.append(f\"Streaks: charge {longest_charge_streak} h, discharge {longest_discharge_streak} h.\")\n",
    "            if energy_utilization is not None:\n",
    "                parts.append(f\"Energy util vs P_max: {pct(energy_utilization)}.\")\n",
    "            if fce is not None:\n",
    "                parts.append(f\"FCE: {fce:.2f}.\")\n",
    "            parts.append(\n",
    "                f\"A_t: {pct(frac_A_pos)} charge, {pct(frac_A_neg)} discharge; align {pct(indicator_alignment)}. \"\n",
    "                f\"Windows(Â±{spread_window_past}/{spread_window_future}h): charge {feas_charge} h, discharge {feas_discharge} h. \"\n",
    "                f\"AI={arbitrage_index:.2f}. Kelly align {pct(kelly_alignment)}.\"\n",
    "            )\n",
    "\n",
    "        return \" \".join(parts)\n",
    "\n",
    "    \n",
    "    def _calculate_performance_metrics(\n",
    "        self,\n",
    "        charge_schedule: List[float],\n",
    "        discharge_schedule: List[float],\n",
    "        soc_trajectory: List[float],\n",
    "        prices: np.ndarray,\n",
    "        import_grid: List[float],\n",
    "        export_grid: List[float],\n",
    "        battery_params: BatteryParams\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various performance metrics\"\"\"\n",
    "        \n",
    "        total_charged_kwh = sum(charge_schedule)\n",
    "        total_discharged_kwh = sum(discharge_schedule)\n",
    "        \n",
    "        # Energy metrics\n",
    "        energy_throughput = total_charged_kwh + total_discharged_kwh\n",
    "        cycle_count = total_charged_kwh / battery_params.capacity_kwh if battery_params.capacity_kwh > 0 else 0\n",
    "        \n",
    "        # Financial metrics\n",
    "        charge_cost = sum(c * p / 1000 for c, p in zip(charge_schedule, prices))\n",
    "        discharge_revenue = sum(d * p / 1000 for d, p in zip(discharge_schedule, prices))\n",
    "        arbitrage_profit = discharge_revenue - charge_cost\n",
    "        \n",
    "        # Efficiency metrics\n",
    "        roundtrip_efficiency = (total_discharged_kwh / total_charged_kwh * 100) if total_charged_kwh > 0 else 0\n",
    "        theoretical_efficiency = battery_params.eta_c * battery_params.eta_d * 100\n",
    "        \n",
    "        # Utilization metrics\n",
    "        avg_soc = np.mean(soc_trajectory)\n",
    "        soc_range = max(soc_trajectory) - min(soc_trajectory)\n",
    "        capacity_utilization = soc_range * 100\n",
    "        \n",
    "        return {\n",
    "            \"total_energy_charged_kwh\": float(total_charged_kwh),\n",
    "            \"total_energy_discharged_kwh\": float(total_discharged_kwh),\n",
    "            \"net_energy_flow_kwh\": float(total_charged_kwh - total_discharged_kwh),\n",
    "            \"energy_throughput_kwh\": float(energy_throughput),\n",
    "            \"cycle_count\": float(cycle_count),\n",
    "            \"charging_cost_eur\": float(charge_cost),\n",
    "            \"discharging_revenue_eur\": float(discharge_revenue),\n",
    "            \"arbitrage_profit_eur\": float(arbitrage_profit),\n",
    "            \"realized_roundtrip_efficiency_percent\": float(roundtrip_efficiency),\n",
    "            \"theoretical_roundtrip_efficiency_percent\": float(theoretical_efficiency),\n",
    "            \"average_soc\": float(avg_soc),\n",
    "            \"soc_range_utilized\": float(soc_range),\n",
    "            \"capacity_utilization_percent\": float(capacity_utilization)\n",
    "        }\n",
    "    \n",
    "    def _validate_solution(\n",
    "        self,\n",
    "        charge_schedule: List[float],\n",
    "        discharge_schedule: List[float],\n",
    "        soc_trajectory: List[float],\n",
    "        battery_params: BatteryParams\n",
    "    ) -> bool:\n",
    "        \"\"\"Validate that solution satisfies all constraints\"\"\"\n",
    "        \n",
    "        # Check SOC bounds\n",
    "        soc_valid = all(\n",
    "            battery_params.soc_min - 1e-6 <= s <= battery_params.soc_max + 1e-6 \n",
    "            for s in soc_trajectory\n",
    "        )\n",
    "        \n",
    "        # Check power bounds\n",
    "        power_valid = all(\n",
    "            c <= battery_params.cmax_kw + 1e-6 and \n",
    "            d <= battery_params.dmax_kw + 1e-6\n",
    "            for c, d in zip(charge_schedule, discharge_schedule)\n",
    "        )\n",
    "        \n",
    "        # Check no simultaneous charge/discharge\n",
    "        no_simultaneous = all(\n",
    "            c < 1e-6 or d < 1e-6 \n",
    "            for c, d in zip(charge_schedule, discharge_schedule)\n",
    "        )\n",
    "        \n",
    "        return soc_valid and power_valid and no_simultaneous\n",
    "    \n",
    "    def _get_season(self, date_str: str) -> str:\n",
    "        \"\"\"Determine season from date\"\"\"\n",
    "        month = datetime.strptime(date_str, \"%Y-%m-%d\").month\n",
    "        if month in [12, 1, 2]:\n",
    "            return \"winter\"\n",
    "        elif month in [3, 4, 5]:\n",
    "            return \"spring\"\n",
    "        elif month in [6, 7, 8]:\n",
    "            return \"summer\"\n",
    "        else:\n",
    "            return \"fall\"\n",
    "        \n",
    "\n",
    "def generate_dataset_for_region(\n",
    "    dataset_name: str,\n",
    "    split: str,\n",
    "    battery_params: BatteryParams,\n",
    "    data_dir: str = \"./data\",\n",
    "    output_dir: str = \"./datasets\",\n",
    "    max_days: int = None\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate fine-tuning dataset for a specific region and split\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: 'italy', 'germany', or 'caiso'\n",
    "        split: 'train' or 'test'\n",
    "        battery_params: Battery configuration\n",
    "        data_dir: Directory containing CSV files\n",
    "        output_dir: Directory to save output datasets\n",
    "        max_days: Maximum number of days to process (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        List of dataset entries\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GENERATING DATASET: {dataset_name.upper()} - {split.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    loader = MultiDatasetLoader(data_dir=data_dir)\n",
    "    df = loader.load_dataset(dataset_name, split)\n",
    "    daily_batches = loader.get_daily_batches(df)\n",
    "    \n",
    "    if max_days:\n",
    "        daily_batches = daily_batches[:max_days]\n",
    "        print(f\"Limiting to first {max_days} days\")\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = DatasetGenerator(battery_config={})\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = []\n",
    "    failures = []\n",
    "    \n",
    "    for idx, (date, prices, demand) in enumerate(daily_batches):\n",
    "        try:\n",
    "            # Create day inputs\n",
    "            day = DayInputs(\n",
    "                prices_buy=prices.tolist(),\n",
    "                demand_kw=demand.tolist(),\n",
    "                prices_sell=prices.tolist(),\n",
    "                allow_export=False,\n",
    "                dt_hours=1.0\n",
    "            )\n",
    "            \n",
    "            # Run MILP solver\n",
    "            milp_solution = solve_daily_milp(\n",
    "                batt=battery_params,\n",
    "                day=day,\n",
    "                solver=None, \n",
    "                solver_opts={}\n",
    "            )\n",
    "\n",
    "# # Run MILP solver\n",
    "# milp_solution = solve_daily_milp(\n",
    "#     batt=battery_params,\n",
    "#     day=day,\n",
    "#     solver=\"GUROBI\", \n",
    "#     solver_opts={}\n",
    "# )\n",
    "            \n",
    "            # Check if solution is valid\n",
    "            if milp_solution.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
    "                print(f\"  Warning: Day {date} - MILP status: {milp_solution.status}\")\n",
    "                failures.append({'date': date, 'reason': milp_solution.status})\n",
    "                continue\n",
    "            \n",
    "            # Create dataset entry\n",
    "            input_str = generator.create_input_component(\n",
    "                date=date,\n",
    "                prices=prices,\n",
    "                demand=demand,\n",
    "                battery_params=battery_params,\n",
    "                dataset_name=dataset_name\n",
    "            )\n",
    "            \n",
    "            output_str = generator.create_output_component(\n",
    "                milp_solution=milp_solution,\n",
    "                prices=prices,\n",
    "                demand=demand,\n",
    "                battery_params=battery_params\n",
    "            )\n",
    "            \n",
    "            dataset.append({\n",
    "                'instruction': generator.instruction,\n",
    "                'input': input_str,\n",
    "                'output': output_str\n",
    "            })\n",
    "            \n",
    "            # Update battery SOC for next day\n",
    "            if milp_solution.soc and len(milp_solution.soc) > 0:\n",
    "                battery_params.soc_init = milp_solution.soc[-1]\n",
    "            \n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(daily_batches)} days\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing day {date}: {e}\")\n",
    "            failures.append({'date': date, 'reason': str(e)})\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GENERATION COMPLETE: {dataset_name.upper()} - {split.upper()}\")\n",
    "    print(f\"  Successful: {len(dataset)} examples\")\n",
    "    print(f\"  Failed: {len(failures)} examples\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Save dataset\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    output_file = output_path / f\"{dataset_name}_{split}.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"Dataset saved to: {output_file}\")\n",
    "    print(f\"Size: {len(json.dumps(dataset)) / 1024 / 1024:.2f} MB\\n\")\n",
    "    \n",
    "    # Save failures log if any\n",
    "    if failures:\n",
    "        failures_file = output_path / f\"{dataset_name}_{split}_failures.json\"\n",
    "        with open(failures_file, 'w') as f:\n",
    "            json.dump(failures, f, indent=2)\n",
    "        print(f\"Failures logged to: {failures_file}\\n\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def combine_datasets(\n",
    "    datasets: List[List[Dict]],\n",
    "    output_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine multiple datasets into one\n",
    "    \"\"\"\n",
    "    print(f\"\\nCombining {len(datasets)} datasets...\")\n",
    "    \n",
    "    combined = []\n",
    "    for ds in datasets:\n",
    "        combined.extend(ds)\n",
    "    \n",
    "    # Shuffle\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(combined)\n",
    "    \n",
    "    # Save\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(combined, f, indent=2)\n",
    "    \n",
    "    print(f\"Combined dataset saved to: {output_path}\")\n",
    "    print(f\"Total examples: {len(combined)}\")\n",
    "    print(f\"Size: {len(json.dumps(combined)) / 1024 / 1024:.2f} MB\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generate fine-tuning datasets from MILP solutions\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        choices=['italy', 'germany', 'caiso', 'all'],\n",
    "        default='all',\n",
    "        help=\"Which dataset to generate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split\",\n",
    "        type=str,\n",
    "        choices=['train', 'test', 'both'],\n",
    "        default='both',\n",
    "        help=\"Which split to generate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--data-dir\",\n",
    "        type=str,\n",
    "        default=\"./data\",\n",
    "        help=\"Directory containing CSV files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        type=str,\n",
    "        default=\"./datasets\",\n",
    "        help=\"Directory to save output datasets\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-days\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Maximum number of days per dataset (for testing)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--generate-all\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Generate all datasets (all regions, train and test)\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Define battery configuration\n",
    "    # Adjust these based on your actual battery specs\n",
    "    battery_params = BatteryParams(\n",
    "        capacity_kwh=21.89,\n",
    "        cmax_kw=5.47,\n",
    "        dmax_kw=5.47,\n",
    "        eta_c=0.95,\n",
    "        eta_d=0.95,\n",
    "        soc_init=0.5,\n",
    "        soc_min=0.0,\n",
    "        soc_max=1.0,\n",
    "        soc_target=0.5\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENERGY STORAGE DATASET GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nBattery Configuration:\")\n",
    "    print(f\"  Capacity: {battery_params.capacity_kwh} kWh\")\n",
    "    print(f\"  Max Charge: {battery_params.cmax_kw} kW\")\n",
    "    print(f\"  Max Discharge: {battery_params.dmax_kw} kW\")\n",
    "    print(f\"  Efficiency: {battery_params.eta_c}/{battery_params.eta_d}\")\n",
    "    print(f\"  SOC Range: [{battery_params.soc_min}, {battery_params.soc_max}]\")\n",
    "    \n",
    "    # Determine which datasets to generate\n",
    "    if args.generate_all or args.dataset == 'all':\n",
    "        datasets_to_generate = ['italy', 'germany', 'caiso']\n",
    "    else:\n",
    "        datasets_to_generate = [args.dataset]\n",
    "    \n",
    "    # Determine which splits to generate\n",
    "    if args.split == 'both':\n",
    "        splits_to_generate = ['train', 'test']\n",
    "    else:\n",
    "        splits_to_generate = [args.split]\n",
    "    \n",
    "    # Generate datasets\n",
    "    all_train_datasets = []\n",
    "    all_test_datasets = []\n",
    "    \n",
    "    for dataset_name in datasets_to_generate:\n",
    "        # Reset battery SOC for each region\n",
    "        battery_params.soc_init = 0.5\n",
    "        \n",
    "        for split in splits_to_generate:\n",
    "            dataset = generate_dataset_for_region(\n",
    "                dataset_name=dataset_name,\n",
    "                split=split,\n",
    "                battery_params=battery_params,\n",
    "                data_dir=args.data_dir,\n",
    "                output_dir=args.output_dir,\n",
    "                max_days=args.max_days\n",
    "            )\n",
    "            \n",
    "            if split == 'train':\n",
    "                all_train_datasets.append(dataset)\n",
    "            else:\n",
    "                all_test_datasets.append(dataset)\n",
    "    \n",
    "    # Combine all training datasets\n",
    "    if len(all_train_datasets) > 1:\n",
    "        combine_datasets(\n",
    "            all_train_datasets,\n",
    "            f\"{args.output_dir}/combined_train.json\"\n",
    "        )\n",
    "    \n",
    "    # Combine all test datasets\n",
    "    if len(all_test_datasets) > 1:\n",
    "        combine_datasets(\n",
    "            all_test_datasets,\n",
    "            f\"{args.output_dir}/combined_test.json\"\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET GENERATION PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Review generated datasets in:\", args.output_dir)\n",
    "    print(\"  2. Use combined_train.json for fine-tuning\")\n",
    "    print(\"  3. Use combined_test.json or individual test sets for evaluation\")\n",
    "    print(\"  4. Run: python train_with_unsloth.py --dataset ./datasets/combined_train.json\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "from agentic_energy.schemas import BatteryParams\n",
    "\n",
    "battery_params = BatteryParams(\n",
    "    capacity_kwh=49.44,\n",
    "    cmax_kw=12.36,\n",
    "    dmax_kw=12.36,\n",
    "    eta_c=0.95,\n",
    "    eta_d=0.95,\n",
    "    soc_init=0.5,\n",
    "    soc_min=0.0,\n",
    "    soc_max=1.0,\n",
    "    soc_target=0.5\n",
    ")\n",
    "\n",
    "# dataset = generate_dataset_for_region(\n",
    "#     dataset_name='caiso',\n",
    "#     split='train',\n",
    "#     battery_params=battery_params,\n",
    "#     data_dir=\"./agentic_energy/data\",\n",
    "#     output_dir=\"./datasets\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56bbec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb47ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbbada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining 3 datasets...\n",
      "Combined dataset saved to: ./datasets/combined_train.json\n",
      "Total examples: 2329\n",
      "Size: 60.74 MB\n",
      "\n",
      "\n",
      "âœ… Combined dataset created!\n",
      "   - Italy: 364 examples\n",
      "   - Germany: 1333 examples\n",
      "   - CAISO: 632 examples\n",
      "   - Total: 2329 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the three individual datasets\n",
    "with open('./datasets/italy_train.json', 'r') as f:\n",
    "    italy_train = json.load(f)\n",
    "\n",
    "with open('./datasets/germany_train.json', 'r') as f:\n",
    "    germany_train = json.load(f)\n",
    "\n",
    "with open('./datasets/caiso_train.json', 'r') as f:\n",
    "    caiso_train = json.load(f)\n",
    "\n",
    "# Combine them\n",
    "combine_datasets(\n",
    "    datasets=[italy_train, germany_train, caiso_train],\n",
    "    output_path='./datasets/combined_train.json'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Combined dataset created!\")\n",
    "print(f\"   - Italy: {len(italy_train)} examples\")\n",
    "print(f\"   - Germany: {len(germany_train)} examples\")\n",
    "print(f\"   - CAISO: {len(caiso_train)} examples\")\n",
    "print(f\"   - Total: {len(italy_train) + len(germany_train) + len(caiso_train)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04371765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d09ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "# from datasets import Dataset, load_dataset\n",
    "# from transformers import TrainingArguments\n",
    "# from trl import SFTTrainer\n",
    "# from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10354f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentics-py (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
